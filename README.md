# Local Memory MCP Server (Project Memory Bank)

[English](#english) | [ä¸­æ–‡è¯´æ˜](#chinese)

---

<a name="english"></a>
## ğŸ‡¬ğŸ‡§ English Description

### ğŸŒŸ Design Philosophy & Motivation

**"Your memory belongs to you, not the cloud."**

I built this project with a simple yet powerful goal: **Total Data Sovereignty**.
In an era of subscription-based AI services and cloud dependencies, I wanted a solution that is:

1.  **100% Local & Private:** No data ever leaves your machine. No API fees, no privacy risks.
2.  **Permanent:** As long as your hard drive exists, your AI's memory exists. No fear of service shutdowns.
3.  **Infinite Capacity:** The only limit is your local disk space.
4.  **High Performance:** Utilizing local GPU acceleration (TensorRT/CUDA) for lightning-fast embedding and retrieval.

This is a **Memory Context Protocol (MCP)** server that gives your AI (like Gemini CLI, Claude Desktop) a persistent, searchable, and evolving long-term memory.

### âœ¨ Key Features

*   **Hybrid Search Architecture:** Combines **LanceDB** (Vector Search for semantic understanding) and **SQLite FTS5** (Full-Text Search for exact keyword matching) for high-precision recall.
*   **Hardware Acceleration:** Powered by ONNX Runtime with TensorRT/CUDA execution providers for millisecond-level embedding generation.
*   **Standard MCP Tools:**
    *   `save_memory`: Store snippets, code, docs, or personal facts.
    *   `search_memory`: Semantic & keyword retrieval.
    *   `list_memories`: View recent entries.
    *   `delete_memory`: Manage and clean up data.
*   **Lazy Loading:** Optimized startup time with on-demand resource initialization.
*   **Zero Cost:** Runs entirely on your existing hardware.

### ğŸ› ï¸ Prerequisites

*   **OS:** Windows (tested)
*   **Python:** 3.10 or higher.
*   **Hardware:** NVIDIA GPU recommended (for TensorRT/CUDA acceleration), but works on CPU.
*   **MCP Client:** [Gemini CLI](https://github.com/google-gemini/gemini-cli) or [Claude Desktop](https://claude.ai/download).Or any IDE that can be configured with MCP.

### ğŸš€ Installation & Setup

#### 1. Clone the Repository
```bash
git clone https://github.com/YanZiBin/Local-memory-mcp.git
cd local-memory-mcp
```

#### 2. Create a Python Environment (Conda Recommended)
To ensure GPU libraries work correctly, Conda is highly recommended.
```bash
conda create -n local-memory python=3.10
conda activate local-memory
```

#### 3. Install Dependencies
```bash
pip install fastmcp lancedb onnxruntime-gpu transformers numpy uvicorn
```
*(Note: If you don't have a GPU, install `onnxruntime` instead of `onnxruntime-gpu`)*

#### 4. Download the Embedding Model
This project uses `BAAI/bge-m3` converted to ONNX. You need to download the model files into the `bge-m3-onnx` directory.

You can use `huggingface-cli` or manually download these files:
*   `sentence_transformers.onnx`
*   `tokenizer.json`
*   `tokenizer_config.json`
*   `vocab.txt`
*   `special_tokens_map.json`

Place them inside a folder named `bge-m3-onnx` in the project root.

### ğŸƒâ€â™‚ï¸ Running the Server

Since this server uses heavy local models, we recommend the **Manual Start (SSE Mode)** for stability.

1.  **Start the Server:**
    Open a terminal and run:
    ```bash
    python server.py
    ```
    Wait until you see: `INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)`

2.  **Connect your Client (e.g., Gemini CLI):**
    
    Edit your Gemini CLI configuration file (usually at `~/.gemini/settings.json` or `%USERPROFILE%\.gemini\settings.json` on Windows):

    ```json
    {
      "mcpServers": {
        "local-memory": {
          "url": "http://localhost:8000/sse",
          "type": "sse"
        }
      }
    }
    ```

3.  **Start using it!**
    Open Gemini CLI and try:
    > "Save this memory: My project uses Python 3.10."
    > "Search my memories for 'project'."

### ğŸ—ºï¸ Roadmap

We are currently at **Phase 2 (Persistence)**.

- [x] **Phase 1: Prototype**
    - Initialize MCP server with `fastmcp`.
    - Basic in-memory storage (dict).
    - Implement basic `save_memory` & `search_memory` (keyword matching).
    - Manual connection testing with Gemini CLI.

- [x] **Phase 2: Persistence (Current)**
    - [x] Integrate **SQLite FTS5** and **LanceDB**.
    - [x] Integrate local **ONNX embedding model**.
    - [x] Dual-storage architecture (Full-text + Vector).
    - [x] `list_memories` and `delete_memory` tools.

- [ ] **Phase 3: Intelligent Retrieval**
    - [ ] Implement **RRF (Reciprocal Rank Fusion)** algorithm.
    - [ ] Add similarity thresholds & Top-K limits.
    - [ ] Implement **Contextual Retrieval** (Enhanced context storage).
    - [ ] (Optional) Add Reranker for higher precision.

- [ ] **Phase 4: Memory Management**
    - [ ] Lifecycle management (Clustering deduplication, Time decay, Conflict tagging).
    - [ ] Support **Channels** (Switching memory context by Git branch).

- [ ] **Phase 5: Advanced Optimization**
    - [ ] Pre-storage summarization/deduplication using local LLM (e.g., Ollama).
    - [ ] Expose **Resources**: Project summaries, ADR (Architecture Decision Records) guardrails.
    - [ ] Architectural guardrails (Recall ADRs on violation) & Task chain tracking.

---

<a name="chinese"></a>
## ğŸ‡¨ğŸ‡³ ä¸­æ–‡è¯´æ˜

### ğŸŒŸ è®¾è®¡åˆè¡·

**â€œä½ çš„è®°å¿†å±äºä½ ï¼Œè€Œä¸æ˜¯äº‘ç«¯ã€‚â€**

å¼€å‘è¿™ä¸ªé¡¹ç›®çš„åˆè¡·éå¸¸çº¯ç²¹ï¼šå®ç°**å®Œå…¨çš„æ•°æ®ä¸»æƒ**ã€‚
åœ¨è¿™ä¸ªä¸‡ç‰©è®¢é˜…åˆ¶ã€éšç§æ‹…å¿§æ—¥ç›Šä¸¥é‡çš„æ—¶ä»£ï¼Œæˆ‘å¸Œæœ›æ„å»ºä¸€ä¸ªè¿™æ ·çš„è§£å†³æ–¹æ¡ˆï¼š

1.  **å®Œå…¨æœ¬åœ°åŒ– & éšç§å®‰å…¨ï¼š** æ²¡æœ‰ä»»ä½•æ•°æ®ä¼šä¸Šä¼ äº‘ç«¯ã€‚æ²¡æœ‰ API è°ƒç”¨è´¹ï¼Œæ²¡æœ‰éšç§æ³„éœ²é£é™©ã€‚
2.  **æ°¸ä¹…å­˜å‚¨ï¼š** åªè¦ä½ çš„ç¡¬ç›˜è¿˜åœ¨ï¼Œä½ çš„ AI è®°å¿†å°±åœ¨ã€‚ä¸å¿…æ‹…å¿ƒæœåŠ¡å•†å€’é—­æˆ–â€œè·‘è·¯â€ã€‚
3.  **æ— é™å®¹é‡ï¼š** å”¯ä¸€çš„é™åˆ¶æ˜¯ä½ æœ¬åœ°ç¡¬ç›˜çš„å¤§å°ï¼ˆç›¸å½“äºæ— é™ï¼‰ã€‚
4.  **æè‡´æ€§èƒ½ï¼š** åˆ©ç”¨æœ¬åœ° GPU åŠ é€Ÿï¼ˆTensorRT/CUDAï¼‰ï¼Œå®ç°æ¯«ç§’çº§çš„è®°å¿†å­˜å–ã€‚

è¿™æ˜¯ä¸€ä¸ª **MCP (Model Context Protocol)** æœåŠ¡å™¨ï¼Œå®ƒä¸ºä½ çš„ AI å·¥å…·ï¼ˆå¦‚ Gemini CLI, Claude Desktopï¼‰æä¾›äº†ä¸€ä¸ªæŒä¹…åŒ–ã€å¯æœç´¢ã€ä¸æ–­è¿›åŒ–çš„â€œå¤–è„‘â€ã€‚

### âœ¨ æ ¸å¿ƒåŠŸèƒ½

*   **æ··åˆæœç´¢æ¶æ„ï¼š** ç»“åˆäº† **LanceDB**ï¼ˆå‘é‡æœç´¢ï¼Œç†è§£è¯­ä¹‰ï¼‰å’Œ **SQLite FTS5**ï¼ˆå…¨æ–‡æœç´¢ï¼Œç²¾å‡†åŒ¹é…å…³é”®è¯ï¼‰ï¼Œç¡®ä¿å¬å›ç‡å’Œå‡†ç¡®ç‡ã€‚
*   **ç¡¬ä»¶åŠ é€Ÿï¼š** åŸºäº ONNX Runtime å’Œ TensorRT/CUDAï¼Œå……åˆ†é‡Šæ”¾æœ¬åœ°æ˜¾å¡æ€§èƒ½ã€‚
*   **æ ‡å‡† MCP å·¥å…·é›†ï¼š**
    *   `save_memory`: ä¿å­˜ä»£ç ç‰‡æ®µã€æ–‡æ¡£æ€»ç»“æˆ–ä¸ªäººäº‹å®ã€‚
    *   `search_memory`: è¯­ä¹‰æˆ–å…³é”®è¯æ£€ç´¢ã€‚
    *   `list_memories`: æŸ¥çœ‹æœ€è¿‘çš„è®°å¿†ã€‚
    *   `delete_memory`: åˆ é™¤è¿‡æ—¶ä¿¡æ¯ã€‚
*   **æ‡’åŠ è½½è®¾è®¡ (Lazy Loading)ï¼š** ä¼˜åŒ–å¯åŠ¨æµç¨‹ï¼ŒæŒ‰éœ€åŠ è½½é‡å‹æ¨¡å‹ï¼Œæ‹’ç»å¡é¡¿ã€‚
*   **é›¶æˆæœ¬ï¼š** ä»¥å‰éœ€è¦ä»˜è´¹è´­ä¹°çš„å‘é‡å­˜å‚¨æœåŠ¡ï¼Œç°åœ¨å…è´¹è¿è¡Œåœ¨ä½ è‡ªå·±çš„ç”µè„‘ä¸Šã€‚

### ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

*   **æ“ä½œç³»ç»Ÿï¼š** Windows (å·²å……åˆ†æµ‹è¯•)
*   **Pythonï¼š** 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚
*   **ç¡¬ä»¶ï¼š** æ¨èä½¿ç”¨ NVIDIA æ˜¾å¡ï¼ˆä»¥è·å¾— TensorRT/CUDA åŠ é€Ÿï¼‰ï¼Œä½†ä¹Ÿæ”¯æŒ CPU è¿è¡Œã€‚
*   **MCP å®¢æˆ·ç«¯ï¼š** [Gemini CLI](https://github.com/google-gemini/gemini-cli) æˆ– [Claude Desktop](https://claude.ai/download)ã€‚æˆ–è€…ä»»ä½•å¯ä»¥é…ç½®mcpçš„IDEã€‚

### ğŸš€ å®‰è£…ä¸é…ç½®æŒ‡å—

#### 1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/YanZiBin/Local-memory-mcp.git
cd local-memory-mcp
```

#### 2. åˆ›å»º Python ç¯å¢ƒ (å¼ºçƒˆæ¨è Conda)
ä¸ºäº†é¿å… CUDA ä¾èµ–å†²çªï¼Œå»ºè®®ä½¿ç”¨ Condaã€‚
```bash
conda create -n local-memory python=3.10
conda activate local-memory
```

#### 3. å®‰è£…ä¾èµ–åº“
```bash
pip install fastmcp lancedb onnxruntime-gpu transformers numpy uvicorn
```
*ï¼ˆæ³¨ï¼šå¦‚æœä½ æ²¡æœ‰ NVIDIA æ˜¾å¡ï¼Œè¯·å°† `onnxruntime-gpu` æ›¿æ¢ä¸º `onnxruntime`ï¼‰*

#### 4. ä¸‹è½½åµŒå…¥æ¨¡å‹ (Embedding Model)
æœ¬é¡¹ç›®ä½¿ç”¨ `BAAI/bge-m3` çš„ ONNX é‡åŒ–ç‰ˆæœ¬ã€‚ä½ éœ€è¦å°†æ¨¡å‹æ–‡ä»¶ä¸‹è½½åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ `bge-m3-onnx` æ–‡ä»¶å¤¹ä¸­ã€‚

ä½ å¯ä»¥ä½¿ç”¨ `huggingface-cli` æˆ–æ‰‹åŠ¨ä¸‹è½½ä»¥ä¸‹æ–‡ä»¶ï¼š
*   `sentence_transformers.onnx`
*   `tokenizer.json`
*   `tokenizer_config.json`
*   `vocab.txt`
*   `special_tokens_map.json`

ç¡®ä¿å®ƒä»¬éƒ½åœ¨ `bge-m3-onnx` æ–‡ä»¶å¤¹å†…ã€‚

### ğŸƒâ€â™‚ï¸ è¿è¡Œä¸ä½¿ç”¨

ç”±äºæœ¬é¡¹ç›®åŠ è½½äº†æœ¬åœ°å¤§æ¨¡å‹ï¼Œä¸ºäº†ç¨³å®šæ€§ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ **æ‰‹åŠ¨å¯åŠ¨ (SSE æ¨¡å¼)**ã€‚

1.  **å¯åŠ¨æœåŠ¡å™¨ï¼š**
    æ‰“å¼€ç»ˆç«¯ï¼ˆCMD/PowerShellï¼‰ï¼Œè¿è¡Œï¼š
    ```bash
    python server.py
    ```
    ç­‰å¾…ç›´åˆ°çœ‹åˆ°æç¤ºï¼š`INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)`

2.  **è¿æ¥å®¢æˆ·ç«¯ (ä»¥ Gemini CLI ä¸ºä¾‹)ï¼š**
    
    ç¼–è¾‘ä½ çš„ Gemini CLI é…ç½®æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº `~/.gemini/settings.json` æˆ– Windows çš„ `%USERPROFILE%\.gemini\settings.json`ï¼‰ï¼š

    ```json
    {
      "mcpServers": {
        "local-memory": {
          "url": "http://localhost:8000/sse",
          "type": "sse"
        }
      }
    }
    ```

3.  **å¼€å§‹ä½“éªŒï¼**
    æ‰“å¼€ Gemini CLIï¼Œç›´æ¥å¯¹è¯ï¼š
    > â€œå¸®æˆ‘è®°ä½ï¼šæˆ‘çš„é¡¹ç›®è¿è¡Œåœ¨ Python 3.10 ç¯å¢ƒä¸‹ã€‚â€
    > â€œæœç´¢è®°å¿†ï¼šå…³äºé¡¹ç›®ç¯å¢ƒçš„ä¿¡æ¯ã€‚â€

### ğŸ—ºï¸ å¼€å‘è·¯çº¿å›¾ (Roadmap)

ç›®å‰é¡¹ç›®å¤„äº **ç¬¬äºŒé˜¶æ®µï¼šæŒä¹…åŒ–**ã€‚

- [x] **ç¬¬ä¸€é˜¶æ®µï¼šåŸå‹éªŒè¯**
    - ä½¿ç”¨ `fastmcp` åˆå§‹åŒ– MCP æœåŠ¡å™¨ã€‚
    - ä½¿ç”¨å†…å­˜å­—å…¸è¿›è¡Œä¸´æ—¶å­˜å‚¨ã€‚
    - å®ç°åŸºç¡€çš„ `save_memory` å’Œ `search_memory`ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰ã€‚
    - é…ç½® stdio å¹¶åœ¨ Gemini CLI ä¸­æµ‹è¯•æ‰‹åŠ¨è¿æ¥ã€‚

- [x] **ç¬¬äºŒé˜¶æ®µï¼šæŒä¹…åŒ–å­˜å‚¨ (å½“å‰é˜¶æ®µ)**
    - [x] å¼•å…¥ **SQLite FTS5** å’Œ **LanceDB**ã€‚
    - [x] é›†æˆæœ¬åœ° **ONNX åµŒå…¥æ¨¡å‹** ç”Ÿæˆå‘é‡ã€‚
    - [x] å®ç°åŒåº“å­˜å‚¨ï¼ˆå…¨æ–‡ + å‘é‡ï¼‰ã€‚
    - [x] æ–°å¢ `list_memories`ï¼ˆåˆ—å‡ºè®°å¿†ï¼‰å’Œ `delete_memory`ï¼ˆåˆ é™¤è®°å¿†ï¼‰å·¥å…·ã€‚

- [ ] **ç¬¬ä¸‰é˜¶æ®µï¼šæ™ºèƒ½ç­›é€‰**
    - [ ] å®ç° **RRF (å€’æ•°æ’åèåˆ)** ç®—æ³•ã€‚
    - [ ] å¢åŠ ç›¸ä¼¼åº¦é˜ˆå€¼å’Œ Top-K é™åˆ¶ï¼ˆé»˜è®¤ 3-5ï¼‰ã€‚
    - [ ] å®ç° **Contextual Retrieval**ï¼ˆå¢å¼ºæ–‡æœ¬å­˜å‚¨ï¼‰ã€‚
    - [ ] ï¼ˆå¯é€‰ï¼‰åŠ å…¥ Reranker é‡æ’åºæ¨¡å‹ä»¥æå‡ç²¾åº¦ã€‚

- [ ] **ç¬¬å››é˜¶æ®µï¼šè®°å¿†ç®¡ç†**
    - [ ] å®ç°ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆåŸºäºç›¸ä¼¼åº¦çš„èšç±»å»é‡ã€æ—¶é—´è¡°å‡ã€å†²çªæ ‡è®°ï¼‰ã€‚
    - [ ] æ”¯æŒæŒ‰ **Git åˆ†æ”¯** åˆ‡æ¢è®°å¿†é¢‘é“ï¼ˆChannelsï¼‰ã€‚

- [ ] **ç¬¬äº”é˜¶æ®µï¼šé«˜çº§ä¼˜åŒ–**
    - [ ] ä½¿ç”¨è§„åˆ™æˆ–æœ¬åœ° LLMï¼ˆå¦‚ Ollamaï¼‰å®ç°å­˜å‚¨å‰çš„æ‘˜è¦/å»é‡ã€‚
    - [ ] æš´éœ² **Resources**ï¼ˆèµ„æºï¼‰ï¼šé¡¹ç›®æ‘˜è¦ã€ADR æ¶æ„å†³ç­–è®°å½•å®ˆæ ç­‰ã€‚
    - [ ] å®ç°æ¶æ„å®ˆæ ï¼ˆè¿è§„æ—¶è‡ªåŠ¨å¬å› ADRï¼‰å’Œä»»åŠ¡é“¾è·Ÿè¸ªã€‚

---

**License:** MIT
**Author:** [YanZiBin]
